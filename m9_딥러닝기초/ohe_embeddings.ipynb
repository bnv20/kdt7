{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNQ5dTAgR3eYB8rDxZLXtRM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9Tw0xGUwbJN","executionInfo":{"status":"ok","timestamp":1668473354073,"user_tz":-540,"elapsed":15821,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"87ee0926-5ca4-423d-d54d-a48575e19822"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Wg3TxQclwPUh","executionInfo":{"status":"ok","timestamp":1668474480473,"user_tz":-540,"elapsed":10,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"outputs":[],"source":["import numpy as np\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","token_index = {}\n","for sample in samples:\n","  for word in sample.split():\n","    if word not in token_index:\n","      token_index[word] = len(token_index) + 1 # 인덱스 0은 사용하지 않음\n","\n","max_length = 10\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1)) # 3차원(2,10,11)\n","\n","for i, sample in enumerate(samples):\n","  for j, word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i,j,index] = 1."]},{"cell_type":"code","source":["results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxfrvdst1CKs","executionInfo":{"status":"ok","timestamp":1668474505096,"user_tz":-540,"elapsed":4,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"e702833d-adb0-41d2-8785-77f42a5bf96f"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["token_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmBant6b1INS","executionInfo":{"status":"ok","timestamp":1668475256988,"user_tz":-540,"elapsed":13,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"18ebdfce-42f7-4506-b3ac-48797e92ca8a"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'The': 1,\n"," 'cat': 2,\n"," 'sat': 3,\n"," 'on': 4,\n"," 'the': 5,\n"," 'mat.': 6,\n"," 'dog': 7,\n"," 'ate': 8,\n"," 'my': 9,\n"," 'homework.': 10}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1,000개의 단어만 선택\n","tokenizer.fit_on_texts(samples)\n","\n","sequences = tokenizer.texts_to_sequences(samples)\n","\n","one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n","\n","word_index = tokenizer.word_index\n","\n","print(word_index)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYwHncoc3_w5","executionInfo":{"status":"ok","timestamp":1668475493601,"user_tz":-540,"elapsed":287,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"65be3564-695d-438e-d989-807a81d2dfad"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"]}]},{"cell_type":"code","source":["one_hot_results[0:2,:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vv9hEzD7483U","executionInfo":{"status":"ok","timestamp":1668475575346,"user_tz":-540,"elapsed":5,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"18224fd6-c28f-4f41-e2e1-c284f77fcc42"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## `Embedding` 층을 사용해 단어 임베딩 학습하기\n","\n","- 단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것이다.\n","- 이 방식의 문제점은 임베딩 공간이 구조적이지 않다는 것입니다. \n","- 예를 들어 accurate와 exact 단어가 대부분 문장에서 비슷한 의미로 사용되지만 완전히 다른 임베딩을 가진다. \n","- 심층 신경망이 이런 임의의 구조적이지 않은 임베딩 공간을 이해하기는 어렵다.\n","- 단어 벡터 사이에 조금 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야 한다.\n","- 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것이다. \n","- 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것이다.\n","- 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계되어 있다(멀리 떨어진 위치에 임베딩된 단어의 의미는 서로 다르고 반면 비슷한 단어들은 가까이 임베딩된다). \n","- 거리외에 임베딩 공간의 특정 방향도 의미를 가질 수 있다.\n","\n","- 실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예는 '성별' 벡터와 '복수(plural)' 벡터이다. \n","- 예를 들어 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 된다. 'plural' 벡터를 더하면 'kings'가 된다. \n","- 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가진다.\n","- 사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 단어 임베딩 공간은 아직 가능하지 않다. 사람의 언어에도 그런 것은 없다. \n","- 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않다. \n","- 실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라진다. - 영어로 된 영화 리뷰 감성 분석 모델을 위한 완벽한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 완벽한 임베딩 공간과 다를 것 이다. 특정 의미 관계의 중요성이 작업에 따라 다르기 때문이다.\n","- 따라서 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당하다. \n","- 다행히 역전파를 사용해 쉽게 만들 수 있고 케라스를 사용하면 더 쉽다. `Embedding` 층의 가중치를 학습하면 된다.\n","\n"],"metadata":{"id":"Fuqbk1HB5Y2S"}},{"cell_type":"markdown","source":["IMDB 영화 max 감성 예측 \n","- max_feature = 10,000개\n","- max_len = 20개 \n","- 이 네트워크는 10,000개의 단어에 대해 8 차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)를 임베딩 시퀀스(3D 실수형 텐서)로 바꿀 것입니다. \n","- 그 다음 이 텐서를 2D로 펼쳐서 분류를 위한 `Dense` 층을 훈련하겠습니다.\n"],"metadata":{"id":"ufJFupxM8Jeg"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras import preprocessing\n","\n","max_features = 10000 # 사용할 단어의 수\n","maxlen = 20 # 사용할 텍스트의 길이\n","\n","(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n","x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen) # 리스트를 (samples,maxlen) 크기의 정수 텐서로 변환\n","x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen) # pad_sequences 길이를 맞춰 줄 때 사용"],"metadata":{"id":"YBTUh45651yB","executionInfo":{"status":"ok","timestamp":1668476913842,"user_tz":-540,"elapsed":5776,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6E0itCp-FBT","executionInfo":{"status":"ok","timestamp":1668476929742,"user_tz":-540,"elapsed":5,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"bdd1feb6-b1a3-4a8a-ec29-13b998e63cff"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 20)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["x_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"526jhex_-YNo","executionInfo":{"status":"ok","timestamp":1668476961663,"user_tz":-540,"elapsed":6,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"16e8780f-c952-4324-bcba-270d8f070d43"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n","        113,  103,   32,   15,   16, 5345,   19,  178,   32], dtype=int32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["x_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VDbK7-g-kSQ","executionInfo":{"status":"ok","timestamp":1668476985822,"user_tz":-540,"elapsed":2,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"46970b60-df06-429e-c01c-64f754c4c7b2"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 20)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","model = Sequential()\n","model.add(Embedding(10000, 8, input_length=maxlen))\n","model.add(Flatten())\n","\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    batch_size=32,\n","                    epochs=10,\n","                    validation_split=0.2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ns59lFHQ-l3_","executionInfo":{"status":"ok","timestamp":1668478576843,"user_tz":-540,"elapsed":25237,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"6d737634-e7e8-4c5f-8a4b-5847a2d499b7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 20, 8)             80000     \n","                                                                 \n"," flatten (Flatten)           (None, 160)               0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 161       \n","                                                                 \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 6s 3ms/step - loss: 0.6650 - acc: 0.6341 - val_loss: 0.6138 - val_acc: 0.7010\n","Epoch 2/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.5395 - acc: 0.7545 - val_loss: 0.5264 - val_acc: 0.7256\n","Epoch 3/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.4622 - acc: 0.7868 - val_loss: 0.5022 - val_acc: 0.7438\n","Epoch 4/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.4219 - acc: 0.8091 - val_loss: 0.4952 - val_acc: 0.7530\n","Epoch 5/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3926 - acc: 0.8236 - val_loss: 0.4966 - val_acc: 0.7542\n","Epoch 6/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3681 - acc: 0.8371 - val_loss: 0.5005 - val_acc: 0.7532\n","Epoch 7/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3451 - acc: 0.8526 - val_loss: 0.5070 - val_acc: 0.7480\n","Epoch 8/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3242 - acc: 0.8636 - val_loss: 0.5151 - val_acc: 0.7454\n","Epoch 9/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3047 - acc: 0.8759 - val_loss: 0.5250 - val_acc: 0.7486\n","Epoch 10/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.2862 - acc: 0.8856 - val_loss: 0.5341 - val_acc: 0.7484\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qJp7E9XHIGE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","\n","zf = zipfile.ZipFile('drive/MyDrive/colab_lecture/datasets/aclImdb.zip')\n","zf.extractall()\n","zf.close()\n"],"metadata":{"id":"Ydw_hstEE0s7","executionInfo":{"status":"ok","timestamp":1668479635169,"user_tz":-540,"elapsed":15702,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6tyj6JnE0sM","executionInfo":{"status":"ok","timestamp":1668479788766,"user_tz":-540,"elapsed":442,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"831b1918-6c44-42b6-fad3-b634bd0d670f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["aclImdb  drive\t__MACOSX  sample_data\n"]}]},{"cell_type":"code","source":["!ls ./aclImdb/train\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igwDXGCNNdye","executionInfo":{"status":"ok","timestamp":1668481400571,"user_tz":-540,"elapsed":327,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"4ff3d17e-10dd-4d19-927a-ad657d4f2dc1"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["neg  pos  urls_neg.txt\turls_pos.txt\n"]}]},{"cell_type":"code","source":["import os\n","\n","imdb_dir = './aclImdb'\n","train_dir = os.path.join(imdb_dir,'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg','pos']:\n","  dir_name = os.path.join(train_dir, label_type)\n","  for fname in os.listdir(dir_name):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname), encoding='utf8')\n","      texts.append(f.read())\n","      f.close()\n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)\n"],"metadata":{"id":"ip0H8rFIPClT","executionInfo":{"status":"ok","timestamp":1668481738328,"user_tz":-540,"elapsed":706,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["texts[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151},"id":"yRvCmVbFQt5L","executionInfo":{"status":"ok","timestamp":1668481748599,"user_tz":-540,"elapsed":287,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"7afe5707-6015-489d-b771-ad36e25512f6"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'For the big thinkers among us, \"The Intruder\" is a maddeningly incoherent movie from France that gives so-called \"art films\" a bad name. The story is something about a bitter old coot, Louis Trebor (Michel Subor), who goes searching in Tahiti for a heart transplant, but beyond that, I have no idea who any of the people in the movie were or why they were doing what they were doing. With no coherent storyline to boast of, the movie loses us early on, though I\\'m perfectly willing to admit that there might be SOMEBODY out there who actually gets some deep message out of this film. <br /><br />This muddled, snail-paced drama runs a full two hours and five minutes - though I seriously doubt anyone with any kind of a life will still be hanging around by the closing credits.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100\n","training_samples = 200\n","validation_samples = 10000\n","max_words = 10000\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts) # 문자 데이터를 입력받아서 리스트의 형태고 변환\n","sequences = tokenizer.texts_to_sequences(texts) # 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n","\n","word_index = tokenizer.word_index\n","print(len(word_index))\n","\n","data = pad_sequences(sequences,maxlen=maxlen)\n","\n","labels = np.asarray(labels)\n","print(data.shape)\n","print(labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kK899LFXQwgZ","executionInfo":{"status":"ok","timestamp":1668482159284,"user_tz":-540,"elapsed":7395,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"5c9a1dad-07c1-43eb-88a4-fa2e3212d06a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["88582\n","(25000, 100)\n","(25000,)\n"]}]},{"cell_type":"code","source":["indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples:training_samples+validation_samples]\n","y_val = labels[training_samples:training_samples+validation_samples]"],"metadata":{"id":"AnaJr9V6SY8I","executionInfo":{"status":"ok","timestamp":1668482376335,"user_tz":-540,"elapsed":321,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["glove_dir = '/content/drive/MyDrive/cakd7/강의/m9_딥러닝기본'\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding='utf8')\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:],dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","print(len(embeddings_index))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-866OdaTJ24","executionInfo":{"status":"ok","timestamp":1668482694226,"user_tz":-540,"elapsed":10246,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"d44930e0-269e-4059-f38b-f3495fafe8ec"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["400000\n"]}]},{"cell_type":"code","source":["embedding_dim = 100\n","embedding_matrix = np.zeros((max_words,embedding_dim))\n","for word, i in word_index.items():\n","  embedding_vector = embeddings_index.get(word)\n","  if i < max_words:\n","    if embedding_vector is not None:\n","      embedding_matrix[i] = embedding_vector"],"metadata":{"id":"kB2uIsRpUb4H","executionInfo":{"status":"ok","timestamp":1668482900581,"user_tz":-540,"elapsed":4,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["embeddings_index.get('the')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"caoEdmD-VK5K","executionInfo":{"status":"ok","timestamp":1668482948847,"user_tz":-540,"elapsed":6,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"dbbbe3ce-bcf4-442d-deb2-3e2f54b8c93e"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n","       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n","        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n","       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n","        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n","       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n","        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n","        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n","       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n","       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n","       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n","       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n","       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n","       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n","       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n","        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n","       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["print(embedding_matrix.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1hf57uOVK4d","executionInfo":{"status":"ok","timestamp":1668482981049,"user_tz":-540,"elapsed":435,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"53bcbe8c-dcb8-4623-f65f-caf3eccd52af"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 100)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words,embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTePhWaQVdhU","executionInfo":{"status":"ok","timestamp":1668483129524,"user_tz":-540,"elapsed":8,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"02a04d7e-4112-4140-bdd1-e2fed9ff96ed"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 100, 100)          1000000   \n","                                                                 \n"," flatten_1 (Flatten)         (None, 10000)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 32)                320032    \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 1,320,065\n","Trainable params: 1,320,065\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# embedding 층 동결\n","model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False"],"metadata":{"id":"Tu8RQFIlWBv1","executionInfo":{"status":"ok","timestamp":1668483193729,"user_tz":-540,"elapsed":385,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer = 'rmsprop',\n","              loss = 'binary_crossentropy',\n","              metrics=['acc'])\n","history = model.fit(x_train,y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_data=(x_val,y_val))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gShaEGRZWSL-","executionInfo":{"status":"ok","timestamp":1668483305511,"user_tz":-540,"elapsed":6736,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"775616b8-bbd4-49b9-c3a2-7d0c8f74beed"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","7/7 [==============================] - 1s 109ms/step - loss: 3.0185 - acc: 0.4100 - val_loss: 0.7069 - val_acc: 0.5270\n","Epoch 2/10\n","7/7 [==============================] - 1s 93ms/step - loss: 0.4925 - acc: 0.8000 - val_loss: 0.9654 - val_acc: 0.4974\n","Epoch 3/10\n","7/7 [==============================] - 1s 95ms/step - loss: 0.5765 - acc: 0.6550 - val_loss: 1.6963 - val_acc: 0.5059\n","Epoch 4/10\n","7/7 [==============================] - 1s 94ms/step - loss: 0.4062 - acc: 0.8250 - val_loss: 1.4392 - val_acc: 0.5073\n","Epoch 5/10\n","7/7 [==============================] - 1s 90ms/step - loss: 0.2016 - acc: 0.9450 - val_loss: 0.9234 - val_acc: 0.5236\n","Epoch 6/10\n","7/7 [==============================] - 1s 89ms/step - loss: 0.0913 - acc: 0.9900 - val_loss: 1.3987 - val_acc: 0.4983\n","Epoch 7/10\n","7/7 [==============================] - 1s 89ms/step - loss: 0.0786 - acc: 0.9900 - val_loss: 1.4373 - val_acc: 0.4985\n","Epoch 8/10\n","7/7 [==============================] - 1s 90ms/step - loss: 0.2928 - acc: 0.8600 - val_loss: 0.8126 - val_acc: 0.5476\n","Epoch 9/10\n","7/7 [==============================] - 1s 89ms/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.8193 - val_acc: 0.5511\n","Epoch 10/10\n","7/7 [==============================] - 1s 90ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.8256 - val_acc: 0.5555\n"]}]},{"cell_type":"code","source":["model.save_weights('drive/MyDrive/colab_lecture/pre_trained_glove_model.h5')"],"metadata":{"id":"npBUFj1pWrMl","executionInfo":{"status":"ok","timestamp":1668483379730,"user_tz":-540,"elapsed":305,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["model.load_weights('drive/MyDrive/colab_lecture/pre_trained_glove_model.h5')"],"metadata":{"id":"7BmGfAE0XBSs","executionInfo":{"status":"ok","timestamp":1668483408846,"user_tz":-540,"elapsed":447,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["### 과제_1115_1\n","다음을 수행하세요.\n","- 훈련 샘플의 수를 2000개로 늘려서 모델링 및 성능 평가"],"metadata":{"id":"NTfGH7XmYKPa"}},{"cell_type":"code","source":[],"metadata":{"id":"wQeGg--WXFx7"},"execution_count":null,"outputs":[]}]}